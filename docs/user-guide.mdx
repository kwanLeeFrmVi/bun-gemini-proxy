# Gemini Proxy Server - Comprehensive User Guide

Welcome to the Gemini Proxy Server! This is a high-performance, production-ready proxy that provides OpenAI-compatible endpoints for multiple AI backends including Google's Gemini API, Gemini CLI, and Codex CLI.

## Table of Contents

1. [Overview](#overview)
2. [Installation & Setup](#installation--setup)
3. [Architecture Overview](#architecture-overview)
4. [Quick Start](#quick-start)
5. [API Endpoints](#api-endpoints)
   - [Main Proxy (/v1/\*)](#main-proxy-v1)
   - [Gemini CLI Backend](#gemini-cli-backend)
   - [Codex CLI Backend](#codex-cli-backend)
6. [Admin Endpoints](#admin-endpoints)
7. [SDK Integration](#sdk-integration)
8. [Configuration Reference](#configuration-reference)
9. [Available Models](#available-models)
10. [Response Formats](#response-formats)
11. [Error Handling](#error-handling)
12. [Advanced Topics](#advanced-topics)
13. [Best Practices](#best-practices)
14. [Troubleshooting](#troubleshooting)
15. [Performance Tuning](#performance-tuning)
16. [Monitoring & Observability](#monitoring--observability)
17. [Support & Resources](#support--resources)

---

## Overview

The Gemini Proxy Server is a **Bun-based TypeScript proxy** that provides:

- **OpenAI-Compatible API** - Drop-in replacement for OpenAI SDK
- **Multi-Backend Support** - Route to Gemini API, Gemini CLI, or Codex CLI
- **Intelligent Key Rotation** - Automatic failover and load balancing
- **Circuit Breaking** - Isolate failing keys to maintain uptime
- **Health Monitoring** - Real-time key health tracking
- **Persistent State** - SQLite-backed state with JSON fallback
- **Production-Ready** - Prometheus metrics, structured logging, hot config reload

### Key Features

- ‚úÖ **Zero-downtime key rotation** with health scoring
- ‚úÖ **Streaming and non-streaming** responses
- ‚úÖ **Automatic retry** with exponential backoff
- ‚úÖ **Sub-100ms proxy overhead** under normal load
- ‚úÖ **Hot configuration reload** without restart
- ‚úÖ **Comprehensive observability** with metrics and structured logs

---

## Installation & Setup

### Prerequisites

- **[Bun](https://bun.sh/)** v1.0.0+ (JavaScript runtime)
- **Gemini API Key** from [Google AI Studio](https://makersuite.google.com/app/apikey)
- **Optional**: `gemini` CLI for Gemini CLI backend
- **Optional**: `codex` CLI for Codex CLI backend

### Quick Start with bunx (Recommended)

Run directly from GitHub without cloning:

1. **Create a working directory**

   ```bash
   mkdir my-gemini-proxy && cd my-gemini-proxy
   ```

2. **Download config templates**

   ```bash
   curl -o proxy.yaml https://raw.githubusercontent.com/your-org/bun-gemini-proxy/main/proxy.example.yaml
   curl -o keys.yaml https://raw.githubusercontent.com/your-org/bun-gemini-proxy/main/keys.example.yaml
   ```

3. **Edit keys.yaml with your API keys**

   ```yaml
   keys:
     - name: "primary-key"
       key: "YOUR_GEMINI_API_KEY_HERE"
       weight: 1
       cooldownSeconds: 30
   ```

4. **Run the proxy**

   ```bash
   bunx github:your-org/bun-gemini-proxy
   ```

5. **Open the user guide**

   The server will display:

   ```
   üöÄ Gemini Proxy Server is running!
   üìñ Open user guide: http://0.0.0.0:8000/help
   ```

   Open http://localhost:8000/help in your browser for full documentation.

### Local Development Installation

For development or customization:

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-org/bun-gemini-proxy.git
   cd bun-gemini-proxy
   ```

2. **Install dependencies**

   ```bash
   bun install
   ```

3. **Configure API keys**

   ```bash
   cp keys.example.yaml keys.yaml
   cp proxy.example.yaml proxy.yaml
   ```

   Edit `keys.yaml`:

   ```yaml
   keys:
     - name: "primary-key"
       key: "YOUR_GEMINI_API_KEY_HERE"
       weight: 1
       cooldownSeconds: 30
     - name: "backup-key"
       key: "YOUR_BACKUP_KEY_HERE"
       weight: 1
       cooldownSeconds: 30
   ```

4. **Configure proxy settings** (optional)

   Edit `proxy.yaml`:

   ```yaml
   proxy:
     host: "0.0.0.0"
     port: 8000
     adminToken: "your-secure-admin-token"
     requestTimeoutMs: 10000
     upstreamBaseUrl: "https://generativelanguage.googleapis.com"
     accessTokens: [] # Empty = no authentication required
     requireAuth: false
   ```

5. **Start the server**

   ```bash
   bun run start
   ```

6. **Verify installation**

   ```bash
   # Check health
   curl http://localhost:8000/health

   # List models
   curl http://localhost:8000/v1/models
   ```

### Development Commands

```bash
# Start server
bun run start

# Run tests
bun test

# Lint code
bun run lint

# Fix linting issues
bun run lint:fix

# Format code
bun run format:fix
```

---

## Architecture Overview

The proxy supports **three backends**, each optimized for different use cases:

### 1. Main Proxy (`/v1/*`) - Production Gemini API

**Best For**: Production workloads, high-throughput applications

**Features**:

- ‚úÖ Intelligent key rotation across multiple Gemini API keys
- ‚úÖ Circuit breaking and automatic failover
- ‚úÖ Health monitoring and recovery
- ‚úÖ Streaming and non-streaming support
- ‚úÖ Full OpenAI API compatibility

**Endpoints**: `/v1/chat/completions`, `/v1/models`, `/v1/embeddings`

**Backend**: Direct HTTP calls to `generativelanguage.googleapis.com`

---

### 2. Gemini CLI Backend (`/gemini-cli/v1/*`)

**Best For**: Local development, testing with GEMINI.md context

**Features**:

- ‚úÖ Uses local `gemini` CLI tool
- ‚úÖ Automatically loads GEMINI.md project context
- ‚úÖ No API key rotation (single CLI session)
- ‚ùå **No streaming support** (CLI limitation)

**Endpoints**: `/gemini-cli/v1/chat/completions`, `/gemini-cli/v1/models`, `/gemini-cli/v1/health`

**Backend**: Executes `gemini` CLI with `--output-format json`

**Requirements**: Install [Gemini CLI](https://github.com/google/gemini-cli)

---

### 3. Codex CLI Backend (`/codex-cli/v1/*`)

**Best For**: Claude-powered agent workflows, tool-using tasks

**Features**:

- ‚úÖ Uses local `codex` CLI tool (Claude Code CLI)
- ‚úÖ Agent-aware responses with reasoning
- ‚úÖ Extended timeout (120s) for complex operations
- ‚úÖ Supports multiple Claude models
- ‚ùå **No streaming support** (CLI limitation)

**Endpoints**: `/codex-cli/v1/chat/completions`, `/codex-cli/v1/models`, `/codex-cli/v1/health`

**Backend**: Executes `codex exec` with `--experimental-json`

**Requirements**: Install [Codex CLI](https://www.anthropic.com/codex)

---

### Backend Selection Guide

| Use Case            | Recommended Backend | Why                                    |
| ------------------- | ------------------- | -------------------------------------- |
| Production API      | `/v1/*`             | Full features, key rotation, streaming |
| Local Gemini dev    | `/gemini-cli/v1/*`  | Project context, no API quota          |
| Claude agent tasks  | `/codex-cli/v1/*`   | Reasoning, tool use, local execution   |
| Testing/prototyping | `/gemini-cli/v1/*`  | No rate limits, fast iteration         |
| High availability   | `/v1/*`             | Circuit breaking, automatic failover   |

---

## Quick Start

### Base URLs

```
Main Proxy:       http://localhost:8000/v1
Gemini CLI:       http://localhost:8000/gemini-cli/v1
Codex CLI:        http://localhost:8000/codex-cli/v1
Admin Interface:  http://localhost:8000/admin
```

### Authentication

**Main Proxy (`/v1/*`)**: Supports optional access token authentication

```bash
# With authentication enabled
curl http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer YOUR_ACCESS_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"model":"gemini-2.0-flash-exp","messages":[{"role":"user","content":"Hello"}]}'
```

**CLI Backends**: No authentication required (local execution)

```bash
# No auth needed
curl http://localhost:8000/codex-cli/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"claude-sonnet-4","messages":[{"role":"user","content":"Hello"}]}'
```

**Admin Endpoints**: Require admin token (configured in `proxy.yaml`)

```bash
curl http://localhost:8000/admin/health \
  -H "Authorization: Bearer test-admin-token"
```

### Configure Authentication

Edit `proxy.yaml`:

```yaml
proxy:
  # Option 1: Disable authentication (development)
  accessTokens: []
  requireAuth: false

  # Option 2: Enable with tokens (production)
  accessTokens:
    - "sk-proxy-token-1"
    - "sk-proxy-token-2"
  requireAuth: true
```

**Note**: When `accessTokens` is empty, authentication is automatically disabled.

---

## API Endpoints

### Main Proxy (/v1/\*)

Full-featured Gemini API proxy with key rotation and circuit breaking.

#### Chat Completions

**POST /v1/chat/completions**

```bash
# Non-streaming
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.0-flash-exp",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is the capital of France?"}
    ],
    "temperature": 0.7,
    "max_tokens": 1000
  }'
```

**Streaming Response**:

```bash
# Streaming with Server-Sent Events (SSE)
curl -N http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.0-flash-exp",
    "messages": [{"role": "user", "content": "Count to 5"}],
    "stream": true
  }'
```

#### List Models

**GET /v1/models**

```bash
curl http://localhost:8000/v1/models
```

Response:

```json
{
  "object": "list",
  "data": [
    {
      "id": "gemini-2.5-pro",
      "object": "model",
      "created": 1234567890,
      "owned_by": "google"
    },
    ...
  ]
}
```

#### Get Model Details

**GET /v1/models/{model_id}**

```bash
curl http://localhost:8000/v1/models/gemini-2.0-flash-exp
```

#### Embeddings

**POST /v1/embeddings**

```bash
curl http://localhost:8000/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": "The quick brown fox jumps over the lazy dog",
    "model": "text-embedding-004"
  }'
```

---

### Gemini CLI Backend

**Base URL**: `http://localhost:8000/gemini-cli/v1`

**Requirements**: Install `gemini` CLI tool

#### Chat Completions

**POST /gemini-cli/v1/chat/completions**

```bash
curl http://localhost:8000/gemini-cli/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-2.5-pro",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

**Supported Models**: `gemini-2.5-pro`, `gemini-2.5-flash`, `gemini-2.0-flash-exp`, `gemini-exp-1206`

**Limitations**:

- ‚ùå No streaming support (`stream: true` returns error)
- ‚è±Ô∏è 60-second timeout (includes GEMINI.md loading)

#### List Models

**GET /gemini-cli/v1/models**

```bash
curl http://localhost:8000/gemini-cli/v1/models
```

#### Health Check

**GET /gemini-cli/v1/health**

```bash
curl http://localhost:8000/gemini-cli/v1/health
```

Response:

```json
{
  "status": "healthy",
  "cli_available": true,
  "cli_version": "gemini-cli 1.2.3"
}
```

---

### Codex CLI Backend

**Base URL**: `http://localhost:8000/codex-cli/v1`

**Requirements**: Install `codex` CLI tool

#### Chat Completions

**POST /codex-cli/v1/chat/completions**

```bash
curl http://localhost:8000/codex-cli/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-sonnet-4",
    "messages": [
      {"role": "user", "content": "Explain quantum computing in simple terms"}
    ]
  }'
```

**Supported Models**:

- `claude-sonnet-4` (default)
- `claude-sonnet-4-5`
- `claude-opus-4`
- `claude-haiku-4`
- `claude-3-5-sonnet-20241022`
- `claude-3-opus-20240229`

**Limitations**:

- ‚ùå No streaming support (`stream: true` returns error)
- ‚è±Ô∏è 120-second timeout (agent operations take longer)

#### List Models

**GET /codex-cli/v1/models**

```bash
curl http://localhost:8000/codex-cli/v1/models
```

#### Health Check

**GET /codex-cli/v1/health**

```bash
curl http://localhost:8000/codex-cli/v1/health
```

Response:

```json
{
  "status": "healthy",
  "cli_available": true,
  "cli_version": "codex-cli 0.42.0"
}
```

---

## Admin Endpoints

Admin endpoints require authentication via `adminToken` configured in `proxy.yaml`.

**Authorization Header**: `Authorization: Bearer YOUR_ADMIN_TOKEN`

### System Health

**GET /admin/health**

```bash
curl http://localhost:8000/admin/health \
  -H "Authorization: Bearer test-admin-token"
```

Response:

```json
{
  "status": "healthy",
  "timestamp": "2025-09-30T12:34:56.789Z",
  "uptime": 3600,
  "keys": {
    "total": 3,
    "healthy": 2,
    "unhealthy": 0,
    "disabled": 1
  }
}
```

**Status Values**:

- `healthy` - All keys operational
- `degraded` - Some keys failing
- `unhealthy` - No healthy keys available

### List Keys

**GET /admin/keys**

```bash
curl http://localhost:8000/admin/keys \
  -H "Authorization: Bearer test-admin-token"
```

Response:

```json
{
  "keys": [
    {
      "name": "primary-key",
      "status": "active",
      "health": 0.95,
      "successCount": 1234,
      "errorCount": 56,
      "lastUsed": "2025-09-30T12:34:56.789Z",
      "lastError": null
    },
    {
      "name": "backup-key",
      "status": "disabled",
      "health": 0.0,
      "successCount": 0,
      "errorCount": 0,
      "lastUsed": null,
      "lastError": null
    }
  ]
}
```

**Status Values**:

- `active` - Key is healthy and in rotation
- `recovering` - Key is in cooldown after failures
- `disabled` - Key manually disabled or permanently failed

### Enable/Disable Keys

**POST /admin/keys/{keyName}/enable**

```bash
curl -X POST http://localhost:8000/admin/keys/backup-key/enable \
  -H "Authorization: Bearer test-admin-token"
```

**POST /admin/keys/{keyName}/disable**

```bash
curl -X POST http://localhost:8000/admin/keys/backup-key/disable \
  -H "Authorization: Bearer test-admin-token"
```

Response:

```json
{
  "success": true,
  "key": "backup-key",
  "status": "disabled"
}
```

### Reload Configuration

**POST /admin/config/reload**

Hot reload configuration from YAML files without restarting the server.

```bash
curl -X POST http://localhost:8000/admin/config/reload \
  -H "Authorization: Bearer test-admin-token"
```

Response:

```json
{
  "success": true,
  "timestamp": "2025-09-30T12:34:56.789Z",
  "message": "Configuration reloaded successfully"
}
```

**Note**: Host/port changes require server restart.

### Prometheus Metrics

**GET /admin/metrics**

```bash
curl http://localhost:8000/admin/metrics \
  -H "Authorization: Bearer test-admin-token"
```

Returns Prometheus-formatted metrics:

```
# HELP proxy_requests_total Total number of proxy requests
# TYPE proxy_requests_total counter
proxy_requests_total{endpoint="/v1/chat/completions",status="200"} 1234

# HELP proxy_request_duration_seconds Request duration in seconds
# TYPE proxy_request_duration_seconds histogram
proxy_request_duration_seconds_bucket{endpoint="/v1/chat/completions",le="0.5"} 100
...
```

---

## SDK Integration

### JavaScript/TypeScript (Bun/Node.js)

```javascript
import OpenAI from "openai";

// Main proxy with key rotation
const mainClient = new OpenAI({
  apiKey: "any-key-works", // Not validated when auth disabled
  baseURL: "http://localhost:8000/v1",
});

// Gemini CLI backend
const geminiClient = new OpenAI({
  apiKey: "not-required",
  baseURL: "http://localhost:8000/gemini-cli/v1",
});

// Codex CLI backend (Claude)
const codexClient = new OpenAI({
  apiKey: "not-required",
  baseURL: "http://localhost:8000/codex-cli/v1",
});

// Non-streaming example
async function nonStreaming() {
  const response = await mainClient.chat.completions.create({
    model: "gemini-2.0-flash-exp",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: "What is TypeScript?" },
    ],
    temperature: 0.7,
    max_tokens: 1000,
  });

  console.log(response.choices[0].message.content);
}

// Streaming example
async function streaming() {
  const stream = await mainClient.chat.completions.create({
    model: "gemini-2.0-flash-exp",
    messages: [{ role: "user", content: "Count to 10" }],
    stream: true,
  });

  for await (const chunk of stream) {
    process.stdout.write(chunk.choices[0]?.delta?.content || "");
  }
}

// Error handling example
async function withErrorHandling() {
  try {
    const response = await mainClient.chat.completions.create({
      model: "gemini-2.0-flash-exp",
      messages: [{ role: "user", content: "Hello!" }],
    });
    console.log(response.choices[0].message.content);
  } catch (error) {
    if (error.status === 503) {
      console.error("All API keys are unhealthy");
    } else if (error.status === 429) {
      console.error("Rate limit exceeded, retry after:", error.headers["retry-after"]);
    } else {
      console.error("Request failed:", error.message);
    }
  }
}
```

### Python

```python
from openai import OpenAI
import time

# Main proxy with key rotation
main_client = OpenAI(
    api_key='any-key-works',
    base_url='http://localhost:8000/v1'
)

# Codex CLI backend
codex_client = OpenAI(
    api_key='not-required',
    base_url='http://localhost:8000/codex-cli/v1'
)

# Non-streaming example
def non_streaming():
    response = main_client.chat.completions.create(
        model='gemini-2.0-flash-exp',
        messages=[
            {'role': 'system', 'content': 'You are a helpful assistant.'},
            {'role': 'user', 'content': 'What is Python?'}
        ],
        temperature=0.7,
        max_tokens=1000
    )
    print(response.choices[0].message.content)

# Streaming example
def streaming():
    stream = main_client.chat.completions.create(
        model='gemini-2.0-flash-exp',
        messages=[{'role': 'user', 'content': 'Count to 5'}],
        stream=True
    )

    for chunk in stream:
        content = chunk.choices[0].delta.content
        if content:
            print(content, end='', flush=True)

# Error handling with retry
def with_retry(max_retries=3):
    for attempt in range(max_retries):
        try:
            response = main_client.chat.completions.create(
                model='gemini-2.0-flash-exp',
                messages=[{'role': 'user', 'content': 'Hello!'}]
            )
            return response.choices[0].message.content
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                print(f"Attempt {attempt + 1} failed, retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise
```

### Go

```go
package main

import (
    "context"
    "fmt"
    "io"

    "github.com/sashabaranov/go-openai"
)

func main() {
    config := openai.DefaultConfig("any-key-works")
    config.BaseURL = "http://localhost:8000/v1"
    client := openai.NewClientWithConfig(config)

    // Non-streaming
    resp, err := client.CreateChatCompletion(
        context.Background(),
        openai.ChatCompletionRequest{
            Model: "gemini-2.0-flash-exp",
            Messages: []openai.ChatCompletionMessage{
                {
                    Role:    openai.ChatMessageRoleUser,
                    Content: "What is Go?",
                },
            },
        },
    )

    if err != nil {
        fmt.Printf("Error: %v\n", err)
        return
    }

    fmt.Println(resp.Choices[0].Message.Content)

    // Streaming
    stream, err := client.CreateChatCompletionStream(
        context.Background(),
        openai.ChatCompletionRequest{
            Model: "gemini-2.0-flash-exp",
            Messages: []openai.ChatCompletionMessage{
                {
                    Role:    openai.ChatMessageRoleUser,
                    Content: "Count to 5",
                },
            },
            Stream: true,
        },
    )

    if err != nil {
        fmt.Printf("Stream error: %v\n", err)
        return
    }
    defer stream.Close()

    for {
        response, err := stream.Recv()
        if err == io.EOF {
            break
        }
        if err != nil {
            fmt.Printf("Stream error: %v\n", err)
            return
        }

        fmt.Print(response.Choices[0].Delta.Content)
    }
}
```

### Ruby

```ruby
require 'openai'

# Main proxy client
client = OpenAI::Client.new(
  access_token: 'any-key-works',
  uri_base: 'http://localhost:8000/v1'
)

# Non-streaming
response = client.chat(
  parameters: {
    model: 'gemini-2.0-flash-exp',
    messages: [
      { role: 'user', content: 'What is Ruby?' }
    ],
    temperature: 0.7
  }
)

puts response.dig('choices', 0, 'message', 'content')

# Streaming
client.chat(
  parameters: {
    model: 'gemini-2.0-flash-exp',
    messages: [{ role: 'user', content: 'Count to 5' }],
    stream: proc do |chunk, _bytesize|
      content = chunk.dig('choices', 0, 'delta', 'content')
      print content if content
    end
  }
)
```

---

## Configuration Reference

### Proxy Configuration (`proxy.yaml`)

```yaml
proxy:
  # Server binding
  host: "0.0.0.0" # Listen on all interfaces
  port: 8000 # HTTP port

  # Admin authentication
  adminToken: "secure-token-here" # Set to null to disable admin auth

  # Request handling
  requestTimeoutMs: 10000 # 10 second timeout per request
  maxPayloadBytes: 10485760 # 10MB payload limit

  # Upstream Gemini API
  upstreamBaseUrl: "https://generativelanguage.googleapis.com"

  # Client authentication (optional)
  accessTokens: # List of valid access tokens
    - "sk-proxy-token-1"
    - "sk-proxy-token-2"
  requireAuth: true # Set to false to disable auth
```

**Authentication Behavior**:

- `accessTokens: []` + `requireAuth: false` ‚Üí No authentication
- `accessTokens: []` + `requireAuth: true` ‚Üí No authentication (auto-disabled)
- `accessTokens: [...]` + `requireAuth: true` ‚Üí Authentication required
- `accessTokens: [...]` + `requireAuth: false` ‚Üí Authentication optional

### API Keys Configuration (`keys.yaml`)

```yaml
keys:
  - name: "production-key-1" # Friendly name for logging
    key: "AIzaSyXXXXXXXXXXXXXXXX" # Gemini API key
    weight: 2 # Higher weight = more traffic (default: 1)
    cooldownSeconds: 30 # Recovery cooldown (default: 30)

  - name: "production-key-2"
    key: "AIzaSyYYYYYYYYYYYYYYYY"
    weight: 1
    cooldownSeconds: 60 # Longer cooldown for rate-limited keys

  - name: "backup-key"
    key: "AIzaSyZZZZZZZZZZZZZZZZ"
    weight: 1
    cooldownSeconds: 30
```

**Key Weight**: Higher weight keys receive more traffic in round-robin rotation.

**Cooldown**: Time in seconds before a failed key is retried.

### Monitoring Configuration (`proxy.yaml`)

```yaml
monitoring:
  healthCheckIntervalSeconds: 30 # Check key health every 30s
  failureThreshold: 3 # Disable key after 3 consecutive failures
  recoveryTimeSeconds: 300 # 5 minutes recovery before re-enabling
  windowSeconds: 300 # 5-minute sliding window for health calc
```

### Persistence Configuration (`proxy.yaml`)

```yaml
persistence:
  sqlitePath: ".runtime/state.sqlite" # Primary state storage
  fallbackJsonPath: ".runtime/state.json" # Backup state storage
```

**State Includes**:

- Key health scores
- Success/failure counters
- Last error timestamps
- Circuit breaker state

---

## Available Models

### Gemini Models (Main Proxy `/v1/*`)

| Model ID               | Description           | Best For                         | Context Window |
| ---------------------- | --------------------- | -------------------------------- | -------------- |
| `gemini-2.5-pro`       | Most capable model    | Complex reasoning, long context  | 2M tokens      |
| `gemini-2.5-flash`     | Fast and efficient    | Quick responses, high throughput | 1M tokens      |
| `gemini-2.0-flash-exp` | Experimental features | Cutting-edge capabilities        | 1M tokens      |
| `gemini-1.5-pro`       | Previous generation   | Stable, production-ready         | 2M tokens      |
| `gemini-1.5-flash`     | Fast previous gen     | Cost-effective, fast             | 1M tokens      |
| `text-embedding-004`   | Text embeddings       | Semantic search, RAG             | N/A            |

### Gemini CLI Models (`/gemini-cli/v1/*`)

Supported: `gemini-2.5-pro`, `gemini-2.5-flash`, `gemini-2.0-flash-exp`, `gemini-exp-1206`

### Codex CLI Models (`/codex-cli/v1/*`)

| Model ID                     | Description        | Best For              |
| ---------------------------- | ------------------ | --------------------- |
| `claude-sonnet-4`            | Balanced (default) | General-purpose tasks |
| `claude-sonnet-4-5`          | Latest Sonnet      | Improved reasoning    |
| `claude-opus-4`              | Most capable       | Complex analysis      |
| `claude-haiku-4`             | Fastest            | Quick responses       |
| `claude-3-5-sonnet-20241022` | Legacy Sonnet      | Compatibility         |
| `claude-3-opus-20240229`     | Legacy Opus        | Compatibility         |

---

## Response Formats

All responses follow OpenAI's format for SDK compatibility.

### Chat Completion Response (Non-Streaming)

```json
{
  "id": "chatcmpl-abc123xyz",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "gemini-2.0-flash-exp",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Response text here"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30
  }
}
```

**Finish Reasons**:

- `stop` - Natural completion
- `length` - Max tokens reached
- `content_filter` - Content policy violation

### Streaming Response (Server-Sent Events)

```
data: {"id":"chatcmpl-abc123","choices":[{"delta":{"role":"assistant","content":"Hello"},"index":0}],"created":1234567890,"model":"gemini-2.0-flash-exp"}

data: {"id":"chatcmpl-abc123","choices":[{"delta":{"content":" world"},"index":0}],"created":1234567890,"model":"gemini-2.0-flash-exp"}

data: {"id":"chatcmpl-abc123","choices":[{"delta":{"content":"!"},"index":0}],"created":1234567890,"model":"gemini-2.0-flash-exp"}

data: {"id":"chatcmpl-abc123","choices":[{"delta":{},"finish_reason":"stop","index":0}],"created":1234567890,"model":"gemini-2.0-flash-exp"}

data: [DONE]
```

### Error Response

```json
{
  "error": {
    "message": "All API keys are unhealthy",
    "type": "service_unavailable",
    "code": 503,
    "details": {
      "healthy_keys": 0,
      "total_keys": 3
    }
  }
}
```

---

## Error Handling

### HTTP Status Codes

| Code | Error Type            | Meaning                | Action                          |
| ---- | --------------------- | ---------------------- | ------------------------------- |
| 400  | Bad Request           | Invalid request format | Fix request payload             |
| 401  | Unauthorized          | Invalid access token   | Check authorization header      |
| 413  | Payload Too Large     | Request exceeds 10MB   | Reduce message size             |
| 429  | Too Many Requests     | Rate limit exceeded    | Retry with exponential backoff  |
| 500  | Internal Server Error | Proxy error            | Check logs, report bug          |
| 502  | Bad Gateway           | Upstream unreachable   | Check network, Gemini status    |
| 503  | Service Unavailable   | All keys unhealthy     | Check key validity, rate limits |

### Common Error Scenarios

#### 503 Service Unavailable

**Cause**: All API keys are unhealthy or disabled

**Troubleshooting**:

1. Check key health: `curl http://localhost:8000/admin/keys`
2. Verify key validity in Google AI Studio
3. Check rate limits and quotas
4. Review logs: `grep ERROR` in server output
5. Manually enable keys: `POST /admin/keys/{keyName}/enable`

#### 413 Payload Too Large

**Cause**: Request body exceeds 10MB limit

**Solution**:

- Reduce message content size
- Split large requests into multiple calls
- Adjust `maxPayloadBytes` in config (not recommended)

#### 429 Too Many Requests

**Cause**: Rate limit exceeded on all keys

**Solution**:

- Implement exponential backoff retry
- Add more API keys to config
- Reduce request frequency
- Check `Retry-After` header for wait time

#### 502 Bad Gateway

**Cause**: Cannot reach upstream Gemini API

**Troubleshooting**:

1. Check network connectivity: `curl https://generativelanguage.googleapis.com`
2. Verify DNS resolution
3. Check firewall/proxy settings
4. Verify `upstreamBaseUrl` in config
5. Check Gemini API status: [status.cloud.google.com](https://status.cloud.google.com)

#### Streaming Not Working

**Cause**: Client not handling SSE properly

**Solution**:

- Use `-N` flag with curl: `curl -N ...`
- Ensure `stream: true` in request body
- Verify OpenAI SDK version supports streaming
- Check network doesn't buffer SSE responses

#### CLI Backend Not Available

**Cause**: CLI tool not installed or not in PATH

**Troubleshooting**:

1. Check CLI availability: `curl http://localhost:8000/codex-cli/v1/health`
2. Verify installation: `which codex` or `which gemini`
3. Add CLI to PATH
4. Restart proxy server after installation

### Debug Logging

Enable verbose logging for troubleshooting:

```bash
# Set log level to debug
LOG_LEVEL=debug bun run start
```

Logs include:

- Request ID for tracing
- Key selection and rotation
- Upstream request/response
- Health score updates
- Circuit breaker events

---

## Advanced Topics

### Load Balancing Strategies

The proxy uses **weighted round-robin** with health-aware selection:

1. **Weight-Based Distribution**: Keys with higher `weight` receive proportionally more traffic
2. **Health Filtering**: Only keys with `status: active` are considered
3. **Circuit Breaking**: Failed keys are temporarily removed from rotation
4. **Automatic Recovery**: Failed keys re-enter rotation after `recoveryTimeSeconds`

**Optimize for**:

- **High Availability**: Set equal weights, add more keys
- **Cost Optimization**: Set higher weight on cheaper/higher-quota keys
- **Performance**: Set higher weight on lower-latency keys

### Rate Limit Management

**Strategy 1: Multiple Keys with Staggered Quotas**

```yaml
keys:
  - name: "peak-hours-key"
    key: "..."
    weight: 3 # 75% of traffic
  - name: "off-peak-key"
    key: "..."
    weight: 1 # 25% of traffic
```

**Strategy 2: Longer Cooldown for Rate-Limited Keys**

```yaml
keys:
  - name: "free-tier-key"
    key: "..."
    cooldownSeconds: 300 # 5 minutes before retry
  - name: "paid-key"
    key: "..."
    cooldownSeconds: 30 # 30 seconds before retry
```

**Strategy 3: Monitor Rate Limit Headers**

The proxy logs rate limit information from upstream responses. Monitor for:

- `X-RateLimit-Remaining`
- `X-RateLimit-Reset`
- `Retry-After`

### Circuit Breaker Tuning

Adjust circuit breaker sensitivity in `proxy.yaml`:

```yaml
monitoring:
  failureThreshold: 3 # Failures before circuit opens
  recoveryTimeSeconds: 300 # Cooldown before retry
  windowSeconds: 300 # Sliding window for health calc
```

**Aggressive (Fast Failover)**:

```yaml
failureThreshold: 2 # Open after 2 failures
recoveryTimeSeconds: 60 # Retry after 1 minute
```

**Conservative (Tolerate Transient Errors)**:

```yaml
failureThreshold: 5 # Open after 5 failures
recoveryTimeSeconds: 600 # Retry after 10 minutes
```

### Performance Optimization

**1. Connection Pooling**

The proxy uses `undici` with connection pooling. Tune for high throughput:

```typescript
// In HttpClient (src/router/client/http-client.ts)
const agent = new Agent({
  connections: 100, // Max concurrent connections
  pipelining: 10, // Requests per connection
  keepAliveTimeout: 60000, // 60s keep-alive
});
```

**2. Timeout Configuration**

Adjust timeouts based on workload:

```yaml
proxy:
  requestTimeoutMs: 30000 # 30s for long-running requests
```

**3. Key Distribution**

For high-volume workloads:

- Use 5+ keys for redundancy
- Set equal weights for even distribution
- Monitor key health in real-time

**4. Payload Optimization**

- Use streaming for large responses
- Compress request payloads client-side
- Reduce `max_tokens` for faster responses

### Hot Configuration Reload

Update configuration without downtime:

```bash
# Edit config files
vim keys.yaml
vim proxy.yaml

# Reload via admin endpoint
curl -X POST http://localhost:8000/admin/config/reload \
  -H "Authorization: Bearer test-admin-token"
```

**Reloadable**:

- ‚úÖ API keys (add/remove/update)
- ‚úÖ Key weights and cooldowns
- ‚úÖ Monitoring thresholds
- ‚úÖ Access tokens
- ‚úÖ Admin token

**Requires Restart**:

- ‚ùå Host/port changes
- ‚ùå Persistence paths
- ‚ùå SSL/TLS settings

---

## Best Practices

### Production Deployment

**Security Checklist**:

- [ ] Enable `requireAuth: true` and set strong access tokens
- [ ] Set secure `adminToken` for admin endpoints
- [ ] Use HTTPS with reverse proxy (nginx, Caddy)
- [ ] Restrict admin endpoints to internal network
- [ ] Rotate API keys regularly
- [ ] Never commit keys to version control

**Reliability Checklist**:

- [ ] Configure 3+ API keys for redundancy
- [ ] Set up health monitoring and alerting
- [ ] Configure appropriate timeouts
- [ ] Enable persistent state (SQLite)
- [ ] Set up automated backups of `.runtime/state.sqlite`
- [ ] Use process manager (PM2, systemd) for auto-restart

**Performance Checklist**:

- [ ] Tune `requestTimeoutMs` for your workload
- [ ] Configure appropriate `failureThreshold`
- [ ] Monitor Prometheus metrics
- [ ] Set up log aggregation (ELK, Loki)
- [ ] Use caching layer if appropriate

### Reverse Proxy Setup (nginx)

```nginx
upstream gemini_proxy {
    server localhost:8000;
}

server {
    listen 443 ssl http2;
    server_name proxy.example.com;

    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;

    location / {
        proxy_pass http://gemini_proxy;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;

        # Support SSE streaming
        proxy_buffering off;
        proxy_cache off;
        chunked_transfer_encoding on;
    }
}
```

### Process Management (systemd)

```ini
# /etc/systemd/system/gemini-proxy.service
[Unit]
Description=Gemini Proxy Server
After=network.target

[Service]
Type=simple
User=gemini-proxy
WorkingDirectory=/opt/gemini-proxy
ExecStart=/usr/local/bin/bun run start
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
```

```bash
# Enable and start
sudo systemctl enable gemini-proxy
sudo systemctl start gemini-proxy

# Check status
sudo systemctl status gemini-proxy

# View logs
journalctl -u gemini-proxy -f
```

### Backup and Recovery

**Backup State Database**:

```bash
# Automated backup script
#!/bin/bash
BACKUP_DIR="/backups/gemini-proxy"
mkdir -p $BACKUP_DIR
cp .runtime/state.sqlite "$BACKUP_DIR/state-$(date +%Y%m%d-%H%M%S).sqlite"
# Keep last 7 days
find $BACKUP_DIR -name "state-*.sqlite" -mtime +7 -delete
```

**Recovery**:

```bash
# Restore from backup
cp /backups/gemini-proxy/state-20250930-120000.sqlite .runtime/state.sqlite
# Restart server
systemctl restart gemini-proxy
```

### Cost Optimization

**Strategy 1: Use Flash Models**

```javascript
// Use gemini-2.5-flash for simple tasks
const response = await client.chat.completions.create({
  model: 'gemini-2.5-flash',  // 10x cheaper than pro
  messages: [...]
});
```

**Strategy 2: Reduce Token Usage**

```javascript
// Limit response length
const response = await client.chat.completions.create({
  model: 'gemini-2.5-pro',
  messages: [...],
  max_tokens: 500,  // Limit output
});
```

**Strategy 3: Cache Responses**

Implement client-side caching for repeated queries:

```javascript
const cache = new Map();

async function cachedCompletion(prompt) {
  if (cache.has(prompt)) {
    return cache.get(prompt);
  }

  const response = await client.chat.completions.create({
    model: "gemini-2.5-flash",
    messages: [{ role: "user", content: prompt }],
  });

  cache.set(prompt, response);
  return response;
}
```

---

## Troubleshooting

### Server Won't Start

**Error: Port 8000 in use**

```bash
# Find process using port
lsof -i :8000

# Kill process
kill -9 <PID>

# Or change port in config
vim proxy.yaml  # Change port: 8000 to port: 8001
```

**Error: Cannot read config files**

```bash
# Check file permissions
ls -la config/

# Fix permissions
chmod 644 *.yaml
```

**Error: SQLite database locked**

```bash
# Stop all proxy instances
pkill -f "bun run start"

# Remove lock file
rm .runtime/state.sqlite-shm
rm .runtime/state.sqlite-wal
```

### Key Health Issues

**All Keys Showing Unhealthy**

1. **Verify API keys**:

   ```bash
   # Test key directly with Gemini API
   curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key=YOUR_KEY" \
     -H "Content-Type: application/json" \
     -d '{"contents":[{"parts":[{"text":"test"}]}]}'
   ```

2. **Check key status in admin**:

   ```bash
   curl http://localhost:8000/admin/keys \
     -H "Authorization: Bearer test-admin-token"
   ```

3. **Manually enable keys**:

   ```bash
   curl -X POST http://localhost:8000/admin/keys/your-key-name/enable \
     -H "Authorization: Bearer test-admin-token"
   ```

4. **Review logs for specific errors**:
   ```bash
   grep "key-name" server.log | grep ERROR
   ```

### Performance Issues

**High Latency**

1. **Check upstream latency**:

   ```bash
   curl http://localhost:8000/admin/metrics | grep upstream_duration
   ```

2. **Increase timeout**:

   ```yaml
   # proxy.yaml
   proxy:
     requestTimeoutMs: 30000 # Increase from 10000
   ```

3. **Add more keys**:
   - More keys = better load distribution
   - Reduces per-key rate limiting

**Memory Leaks**

1. **Monitor memory usage**:

   ```bash
   # Check process memory
   ps aux | grep bun
   ```

2. **Restart server periodically**:

   ```bash
   # Add to cron (restart daily at 3am)
   0 3 * * * systemctl restart gemini-proxy
   ```

3. **Check for streaming connection leaks**:
   - Ensure clients properly close streams
   - Monitor open connections: `lsof -p <PID> | wc -l`

### CLI Backend Issues

**Gemini CLI Not Found**

```bash
# Check if CLI is installed
which gemini

# Install Gemini CLI
# Follow: https://github.com/google/gemini-cli

# Verify health endpoint
curl http://localhost:8000/gemini-cli/v1/health
```

**Codex CLI Not Found**

```bash
# Check if CLI is installed
which codex

# Verify health endpoint
curl http://localhost:8000/codex-cli/v1/health
```

**CLI Timeout Errors**

```bash
# CLI operations can be slow, especially with GEMINI.md loading
# Increase timeout in router if needed
# Or use main proxy (/v1/*) for production workloads
```

### Database Corruption

**Error: Database is corrupt**

```bash
# Backup corrupted database
cp .runtime/state.sqlite .runtime/state.sqlite.corrupt

# Try recovery
sqlite3 .runtime/state.sqlite ".recover" > recovered.sql
sqlite3 .runtime/state-new.sqlite < recovered.sql
mv .runtime/state-new.sqlite .runtime/state.sqlite

# Or delete and recreate (loses history)
rm .runtime/state.sqlite
# Server will create new database on startup
```

---

## Performance Tuning

### Benchmarking

Use Apache Bench for load testing:

```bash
# Test chat completions endpoint
ab -n 1000 -c 10 -T 'application/json' \
  -p request.json \
  http://localhost:8000/v1/chat/completions
```

Sample `request.json`:

```json
{
  "model": "gemini-2.5-flash",
  "messages": [{ "role": "user", "content": "Hello" }]
}
```

### Optimal Configuration for High Throughput

```yaml
# proxy.yaml
proxy:
  requestTimeoutMs: 15000 # 15s timeout
  maxPayloadBytes: 5242880 # 5MB limit (reduce for faster)

monitoring:
  healthCheckIntervalSeconds: 10 # Check health every 10s
  failureThreshold: 2 # Fast failover
  recoveryTimeSeconds: 60 # Quick recovery
  windowSeconds: 60 # Short window for fast adaptation

# keys.yaml
keys:
  # 5+ keys with equal weights for best distribution
  - name: "key-1"
    weight: 1
  - name: "key-2"
    weight: 1
  - name: "key-3"
    weight: 1
  - name: "key-4"
    weight: 1
  - name: "key-5"
    weight: 1
```

### Optimal Configuration for Reliability

```yaml
# proxy.yaml
proxy:
  requestTimeoutMs: 30000 # 30s timeout (tolerate slow responses)

monitoring:
  healthCheckIntervalSeconds: 30 # Less frequent checks
  failureThreshold: 5 # Tolerate transient errors
  recoveryTimeSeconds: 300 # Conservative recovery
  windowSeconds: 600 # Long window for stability
```

### Resource Limits

**Memory Usage**:

- Baseline: ~50-100MB
- Per concurrent request: ~1-5MB
- SQLite database: ~1-10MB

**CPU Usage**:

- Mostly I/O bound
- Minimal CPU for JSON parsing/streaming
- Use multiple instances for CPU-heavy workloads

**Scaling Horizontally**:

```bash
# Run multiple instances on different ports
PORT=8000 bun run start &
PORT=8001 bun run start &
PORT=8002 bun run start &

# Load balance with nginx
upstream gemini_cluster {
    server localhost:8000;
    server localhost:8001;
    server localhost:8002;
}
```

---

## Monitoring & Observability

### Prometheus Metrics

**Request Metrics**:

```
proxy_requests_total{endpoint,status}           # Total requests
proxy_request_duration_seconds{endpoint}        # Request latency
proxy_active_requests{endpoint}                 # Active requests
```

**Key Health Metrics**:

```
proxy_key_health{key_name}                      # Key health score (0-1)
proxy_key_requests_total{key_name,result}       # Per-key request count
proxy_upstream_duration_seconds{endpoint}       # Upstream latency
```

**Sample Prometheus Config**:

```yaml
# prometheus.yml
scrape_configs:
  - job_name: "gemini-proxy"
    static_configs:
      - targets: ["localhost:8000"]
    metrics_path: "/admin/metrics"
    authorization:
      credentials: "test-admin-token"
```

### Grafana Dashboard

**Key Panels**:

1. **Request Rate**: `rate(proxy_requests_total[5m])`
2. **Error Rate**: `rate(proxy_requests_total{status!="200"}[5m])`
3. **Latency p95**: `histogram_quantile(0.95, proxy_request_duration_seconds)`
4. **Key Health**: `proxy_key_health`
5. **Active Keys**: `count(proxy_key_health > 0.8)`

### Alerting Rules

```yaml
# prometheus-alerts.yml
groups:
  - name: gemini_proxy
    rules:
      - alert: AllKeysUnhealthy
        expr: count(proxy_key_health > 0.5) == 0
        for: 5m
        annotations:
          summary: "All API keys are unhealthy"

      - alert: HighErrorRate
        expr: rate(proxy_requests_total{status!="200"}[5m]) > 0.1
        for: 5m
        annotations:
          summary: "Error rate above 10%"

      - alert: HighLatency
        expr: histogram_quantile(0.95, proxy_request_duration_seconds) > 5
        for: 10m
        annotations:
          summary: "P95 latency above 5 seconds"
```

### Structured Logging

Logs are JSON-formatted for easy parsing:

```json
{
  "level": 30,
  "time": 1727708400000,
  "msg": "Request completed",
  "requestId": "abc123",
  "endpoint": "/v1/chat/completions",
  "model": "gemini-2.0-flash-exp",
  "key": "primary-key",
  "duration": 1234,
  "status": 200
}
```

**Parse with jq**:

```bash
# Filter errors
cat server.log | jq 'select(.level >= 50)'

# Average latency
cat server.log | jq -s 'map(.duration) | add/length'

# Count by endpoint
cat server.log | jq -r '.endpoint' | sort | uniq -c
```

---

## Support & Resources

### Documentation

- **User Guide**: This document
- **PRD**: `PRD.md` - Product requirements and architecture
- **Specs**: `specs/` - Detailed specifications
- **API Reference**: `docs/api-reference.md` (if available)

### GitHub Repository

- **Issues**: Report bugs and request features
- **Pull Requests**: Contribute improvements
- **Discussions**: Ask questions and share ideas

### Getting Help

1. **Check Logs**: Review structured logs for error details
2. **Admin Endpoints**: Use `/admin/health` and `/admin/keys` for diagnostics
3. **Health Checks**: Verify CLI backends with `/gemini-cli/v1/health` and `/codex-cli/v1/health`
4. **Configuration**: Review `*.yaml` for misconfigurations
5. **GitHub Issues**: Search existing issues or create new one

### Common Resources

- **Gemini API Docs**: [ai.google.dev/docs](https://ai.google.dev/docs)
- **OpenAI API Reference**: [platform.openai.com/docs/api-reference](https://platform.openai.com/docs/api-reference)
- **Bun Documentation**: [bun.sh/docs](https://bun.sh/docs)
- **Prometheus Docs**: [prometheus.io/docs](https://prometheus.io/docs)

---

## Appendix

### Version History

- **v1.0.0** (2025-09-30)
  - Initial release with Gemini API proxy
  - Key rotation and health monitoring
  - Circuit breaking and failover
  - Gemini CLI backend
  - Codex CLI backend
  - Admin endpoints
  - Prometheus metrics

### License

See `LICENSE` file in repository.

### Contributing

Contributions welcome! See `CONTRIBUTING.md` for guidelines.

---

**Server Version**: 1.0.0
**Last Updated**: 2025-09-30
**Maintainer**: Your Team

For latest updates, visit the [GitHub repository](https://github.com/your-org/bun-gemini-proxy).
